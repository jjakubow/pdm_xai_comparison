{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_files = sorted(list(filter(re.compile(\"explanations.*.csv\").search, os.listdir(\"results\"))))\n",
    "print(\"Found %s explanation files:\" % len(explanation_files))\n",
    "for i, f in enumerate(explanation_files):\n",
    "    print(\"  %i. %s\" % (i+1, f))\n",
    "df = pd.read_csv(os.path.join(\"results\", explanation_files[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Matplotlib settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colormap for the plots and global settings\n",
    "\n",
    "colormap = ListedColormap([\"#D00000\", \"#FFBA08\", \"#3F88C5\", \"#032B43\", \"#136F63\"], name=\"CMAPSS_5\")\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=colormap.colors)\n",
    "plt.rcParams['axes.axisbelow'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "colormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_explanations(df):\n",
    "    n_observations = len(df[[\"unit\", \"RUL\"]].value_counts())\n",
    "    columns = sorted(df[\"Feature\"].unique())\n",
    "    X = pd.DataFrame(df.sort_values(by=[\"unit\", \"RUL\", \"Feature\"])[\"Value\"].values.reshape((n_observations, -1)), columns=columns)\n",
    "    explanation_scores = pd.DataFrame(df.sort_values(by=[\"unit\", \"RUL\", \"Feature\"])[\"Explain_Score\"].values.reshape((n_observations, -1)), columns=columns)\n",
    "    rul = pd.DataFrame(df.sort_values(by=[\"unit\", \"RUL\", \"Feature\"])[[\"unit\", \"RUL\"]]).drop_duplicates()[\"RUL\"]\n",
    "    return X, explanation_scores, rul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScaler:\n",
    "    \"\"\" Custom scaler which scales all features to min and max found in the dataframe.\n",
    "        The scaling may not be done to a certain quantile.\n",
    "        \n",
    "        We want to keep the scale relative, but the explanations should still be cetered around 0 -> min max scaling or other could shift\n",
    "        the explanations and original 0 could become i.e. 0.2, which should be avoided.\n",
    "        Therefore scaling is done in following way:\n",
    "            1. We take all the importances from a given dataframes -> perfectly all dataframes for certain method i.e. shap\n",
    "            2. We calculate the xth quantile (parameter), which will be considered as 1\n",
    "            3. We divide all the values by this parameter -> in such way most relevant features will have importance of around 1\n",
    "               and non-important will still be 0\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, quantile=1.0):\n",
    "        self.q = quantile\n",
    "        self.xmax = None\n",
    "        \n",
    "    \n",
    "    def fit(self, Xs):\n",
    "        X = np.concatenate(Xs)\n",
    "        X_flat_abs = np.abs(X.reshape((-1)))\n",
    "        self.xmax = np.quantile(X_flat_abs, self.q)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X / self.xmax\n",
    "\n",
    "    \n",
    "    def inverse_transform(self, X_scaled):\n",
    "        pass\n",
    "    \n",
    "\n",
    "def build_scaler(files, scaler_dict={}):\n",
    "    exp_dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(os.path.join(\"results\", f))\n",
    "        print(f, len(df))\n",
    "        _f, explanation_scores, _r = get_features_and_explanations(df)\n",
    "        exp_dfs.append(explanation_scores)\n",
    "    scaler = CustomScaler(**scaler_dict)\n",
    "    scaler.fit(exp_dfs)\n",
    "    \n",
    "    return scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global scalers \n",
    "\n",
    "shap_files = [f for f in explanation_files if 'shap' in f]\n",
    "lime_files = [f for f in explanation_files if 'lime' in f]\n",
    "ebm_files = [f for f in explanation_files if 'ebm' in f]\n",
    "\n",
    "shap_scaler = build_scaler(shap_files, scaler_dict={\"quantile\": 0.95})\n",
    "lime_scaler = build_scaler(lime_files, scaler_dict={\"quantile\": 0.95})\n",
    "ebm_scaler = build_scaler(ebm_files, scaler_dict={\"quantile\": 0.95})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "df = pd.read_csv(os.path.join(\"results\", 'xgb_shap_explanations_0208-1641.csv'))\n",
    "X, explanation_scores, rul = get_features_and_explanations(df)\n",
    "explanation_scores_scaled = shap_scaler.transform(explanation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for i in tqdm(range(len(X))):\n",
    "    for j in range(len(X)):\n",
    "        if j <= i:\n",
    "            continue\n",
    "        dist = np.linalg.norm(X.values[i] - X.values[j])\n",
    "        if dist > 0:\n",
    "            distances.append(dist)\n",
    "        \n",
    "plt.hist(distances, bins=50)\n",
    "plt.show()\n",
    "\n",
    "for q in [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25]:\n",
    "    p_q = np.quantile(distances, q)\n",
    "    print(\"%ith qunatile = %.2f\" % (q*100, p_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stability F\n",
    "From https://github.com/sbobek/inxai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stability -> the dist + 1 does not appear to be consistent with the paper\n",
    "\n",
    "def stability(X, all_importances, epsilon=0.3, progress_bar=False):\n",
    "    \"\"\"Stability as Lipschitz coefficient.\n",
    "    :param X:\n",
    "    :param all_importances:\n",
    "    :param epsilon:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    l_values = []\n",
    "\n",
    "    if  not isinstance(all_importances, np.ndarray):\n",
    "        all_importances = np.array(all_importances)\n",
    "\n",
    "    for data_idx, (_, observation) in tqdm(enumerate(X.iterrows()), disable=not progress_bar, total=len(X)):\n",
    "        max_val = 0\n",
    "        for idx, (_, other_observation) in enumerate(X.iterrows()):\n",
    "            dist = np.linalg.norm(observation - other_observation)\n",
    "            \n",
    "            if dist == 0:\n",
    "                # to avoid division by 0 if distance is 0, the observation is omitted\n",
    "                continue\n",
    "            \n",
    "            if dist < epsilon:\n",
    "                delta_imp = np.linalg.norm(all_importances[data_idx] - all_importances[idx])\n",
    "                val =  delta_imp / dist\n",
    "                if val > max_val:\n",
    "                    max_val = val\n",
    "                    \n",
    "        if max_val == 0:\n",
    "            # there were no close points found so we do not include this point in the results\n",
    "            continue\n",
    "        \n",
    "        l_values.append(max_val)\n",
    "        \n",
    "    return np.array(l_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "explanations_stability = stability(X, explanation_scores_scaled, epsilon=3.0, progress_bar=True)\n",
    "plt.hist(explanations_stability)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculations and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Consistency (from inxai / sbobek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def consistency(all_importances_per_model, confidence=None, progress_bar=False):\n",
    "    \"\"\" Calculates maximum distance to explanation generated by the same instance and different models\n",
    "    :param all_importances_per_model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    c_values = []\n",
    "\n",
    "    if  not isinstance(all_importances_per_model, np.ndarray):\n",
    "        all_importances_per_model = np.array(all_importances_per_model)\n",
    "        \n",
    "\n",
    "    if confidence is None:\n",
    "        confidence = np.ones((all_importances_per_model.shape[0],all_importances_per_model.shape[1]))\n",
    "    elif not isinstance(confidence,np.ndarray):\n",
    "        confidence = np.array(confidence).reshape((all_importances_per_model.shape[0],all_importances_per_model.shape[1]))\n",
    "\n",
    "    for obs_idx in tqdm(range(len(all_importances_per_model[0])), disable=not progress_bar):\n",
    "        largest_dist = 0\n",
    "        for model_idx, model_imps in enumerate(all_importances_per_model):\n",
    "            for other_idx, compared_model in enumerate(all_importances_per_model[:model_idx]):\n",
    "                current_imps = model_imps[obs_idx]\n",
    "                other_imps = compared_model[obs_idx]\n",
    "                if not isinstance(current_imps, np.ndarray):\n",
    "                    current_imps = np.array(current_imps)\n",
    "                if not isinstance(other_imps, np.ndarray):\n",
    "                    other_imps = np.array(other_imps)\n",
    "                dist = np.linalg.norm(current_imps - other_imps)*confidence[model_idx,obs_idx]*confidence[other_idx,obs_idx]\n",
    "                if dist > largest_dist:\n",
    "                    largest_dist = dist\n",
    "        c_values.append(1.0/(largest_dist+1))\n",
    "\n",
    "    return c_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "shap_consistency = consistency([explanation_scores_scaled, explanation_scores_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"results/stability_dict.pickle\", \"rb\") as f:\n",
    "        stability_dict = pickle.load(f)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    stability_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'data' dictionary is structured in following way:\n",
    "# key -> this is the name of the explanation model i.e. xgb_lime\n",
    "# value -> this is tuple containing (df, es), where df contains feature values and es contains explanation scores (feature importances)\n",
    "\n",
    "data = {}\n",
    "\n",
    "for f in tqdm(explanation_files):\n",
    "    df = pd.read_csv(os.path.join(\"results\", f))\n",
    "    X, es, rul = get_features_and_explanations(df)\n",
    "    \n",
    "    if \"shap\" in f:\n",
    "        # print(\"Scaling with shap...\")\n",
    "        es_scaled = shap_scaler.transform(es)\n",
    "    elif \"lime\" in f:\n",
    "        # print(\"Scaling with lime...\")\n",
    "        es_scaled = lime_scaler.transform(es)\n",
    "    elif \"ebm\" in f:\n",
    "        # print(\"Scaling with ebm...\")\n",
    "        es_scaled = ebm_scaler.transform(es)\n",
    "    else:\n",
    "        raise ValueError(\"One of explanation methods ('shap', 'lime', 'ebm') must be in file name to assign correct scaler.\")\n",
    "    \n",
    "    # name = f.replace(\"_explanations.csv\", \"\")\n",
    "    name = re.sub(\"_explanations.*.csv\", \"\", f)\n",
    "    data[name] = (X, es_scaled, rul)\n",
    "    \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel computation\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import warnings\n",
    "\n",
    "def calc_stability(data, key):\n",
    "    print(\"Started calculation of stability for %s\" % key)\n",
    "    X, es, rul = data[key]\n",
    "    stab = stability(X, es, epsilon=1.5, progress_bar=True)\n",
    "    print(\"Stability for %s calculated\" % key)\n",
    "    return {key: stab}\n",
    "\n",
    "results = Parallel(n_jobs=6)(delayed(calc_stability)(data, i) for i in data.keys())\n",
    "\n",
    "stability_dict = {}\n",
    "for element in results:\n",
    "    key = list(element.keys())[0]\n",
    "    stability_dict[key] = element[key]\n",
    "\n",
    "with open(r\"results/stability_dict_ebm.pickle\", \"wb\") as f:\n",
    "    pickle.dump(stability_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_dict_to_df(stability_dict):\n",
    "    \"\"\"Transforms stability_dict to dataframe for summary boxplot\"\"\"\n",
    "    df_stability = pd.DataFrame.from_dict(stability_dict).melt(var_name=\"name\", value_name=\"stability\")\n",
    "    df_stability[\"classification_method\"] = df_stability[\"name\"].apply(lambda x: x.split(\"_\")[0].upper())    \n",
    "    df_stability[\"explanation_method\"] = df_stability[\"name\"].apply(lambda x: \"SHAP\" if \"shap\" in x else\n",
    "                                                                     \"LIME\" if \"lime\" in x else\n",
    "                                                                     \"EBM\" if \"ebm\" in x else \"error\")\n",
    "    \n",
    "    return df_stability\n",
    "\n",
    "    \n",
    "df_stability = stability_dict_to_df(stability_dict)\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.rc('axes', axisbelow=True)\n",
    "#plt.gca().yaxis.grid(alpha=1, zorder=1)\n",
    "sns.boxplot(data=df_stability, x=\"classification_method\", y=\"stability\", hue=\"explanation_method\", hue_order=[\"LIME\", \"EBM\", \"SHAP\"], width=0.5,\n",
    "           fliersize=2, linewidth=1)\n",
    "plt.xlabel(None)\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.05), ncol=3, loc=\"center\", frameon=False, title=None, title_fontsize=11)\n",
    "plt.gca().yaxis.grid(alpha=0.8)\n",
    "#plt.yscale('log')\n",
    "plt.ylim(0, )\n",
    "plt.tight_layout()\n",
    "# plt.savefig(r\"plots/models_stability.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Stability vs epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stability_dict_eps = {}\n",
    "\n",
    "epsilons = [0.005, 0.01]\n",
    "# epsilons = [0.05, 0.1]\n",
    "# epsilons = [0.2, 0.3, 0.5]\n",
    "# epsilons = [0.8, 1]\n",
    "\n",
    "\n",
    "for eps in tqdm(epsilons):\n",
    "    for key, result in data.items():\n",
    "        X, es = result\n",
    "        key_eps = key + \"_\" + str(eps)\n",
    "        stability_dict_eps[key_eps] = stability(X, es, epsilon=eps, progress_bar=False)\n",
    "        \n",
    "with open(r\"results/stability_dict_eps.pickle\", \"wb\") as f:\n",
    "    pickle.dump(stability_dict_eps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stability_eps = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "for key, stability_score in stability_dict_eps.items():\n",
    "    mod = re.findall(\"xgb|rf|mlp|svm|ebm\", key)[0]\n",
    "    explain = re.findall(\"shap|lime|ebm\", key)[0]\n",
    "    eps = key.split(\"_\")[-1]\n",
    "    \n",
    "    df_stability_eps.loc[i, \"Model\"] = mod.upper()\n",
    "    df_stability_eps.loc[i, \"Explanation\"] = explain.upper()\n",
    "    df_stability_eps.loc[i, \"Model+Explanation\"] = \"%s+%s\" % (mod.upper(), explain.upper())\n",
    "    df_stability_eps.loc[i, \"Epsilon\"] = float(eps)\n",
    "    df_stability_eps.loc[i, \"Stability\"] = np.mean(stability_score)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "sns.lineplot(data=df_stability_eps.query(\"Explanation == 'SHAP'\"), x=\"Epsilon\", y=\"Stability\", hue=\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_dict = {}\n",
    "\n",
    "for m1, res1 in tqdm(data.items()):\n",
    "    X1, es1, rul1 = res1\n",
    "    for m2, res2 in data.items():\n",
    "        X2, es2, rul2 = res2\n",
    "        # we do not compare the model to itself, as the consistency here would be always 1\n",
    "        if m1 == m2:\n",
    "            continue\n",
    "        \n",
    "        key = (m1, m2)\n",
    "        key_inverse = (m2, m1)\n",
    "        \n",
    "        # if a pair was already calculated, skip it\n",
    "        if key_inverse in consistency_dict.keys():\n",
    "            continue\n",
    "        \n",
    "        consistency_dict[key] =  consistency([es1, es2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency_dict_to_df(consistency_dict, aggregate_method=np.mean):\n",
    "    names = sorted(set(map(lambda x: x[0], consistency_dict.keys())))\n",
    "    names = [n.replace(\"_\", \"+\").upper() for n in names]\n",
    "    df_consistency = pd.DataFrame(index=names, columns=names)\n",
    "    \n",
    "    for name_tup, result in consistency_dict.items():\n",
    "        m1, m2 = name_tup\n",
    "        m1 = m1.replace(\"_\", \"+\").upper()\n",
    "        m2 = m2.replace(\"_\", \"+\").upper()\n",
    "        df_consistency.loc[m1, m2] = aggregate_method(result)\n",
    "    \n",
    "    df_consistency = df_consistency.astype(np.float32)\n",
    "    df_consistency = df_consistency.iloc[:, 1:] # removes first column, which has only NaNs\n",
    "    return df_consistency\n",
    "\n",
    "df_consistency = consistency_dict_to_df(consistency_dict, aggregate_method=np.median)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(df_consistency, cmap=\"viridis\", annot=True ,\n",
    "            cbar_kws={\"label\": \"Consistency\", \"location\": \"top\", \"fraction\": 0.06, \"format\": \"%.2f\"},\n",
    "            vmin=0.2, vmax=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"plots/models_consistency.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = []\n",
    "for key in data.keys():\n",
    "    es = data[key][1]\n",
    "    top_features = es.abs().mean(axis=0).sort_values(ascending=False)\n",
    "    top_features = top_features.index.values[:3]\n",
    "    for feat in top_features:\n",
    "        if feat not in important_features:\n",
    "            important_features.append(feat)\n",
    "\n",
    "df_feature_imp = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "for key in data.keys():\n",
    "    es = data[key][1]\n",
    "    for feat in sorted(important_features):\n",
    "        df_feature_imp.loc[i, \"Explanation\"] = key.replace(\"_\", \"+\").upper()\n",
    "        df_feature_imp.loc[i, \"Feature\"] = feat\n",
    "        df_feature_imp.loc[i, \"Importance\"] = es[feat].abs().mean()\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_palette(colormap.colors, n_colors=5)\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.barplot(hue=\"Feature\", y=\"Importance\", x=\"Explanation\", data=df_feature_imp)\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.05), ncol=5, loc=\"center\", frameon=False, title=None, title_fontsize=11)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Mean relative importance\")\n",
    "#plt.gca().yaxis.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"plots/important_features.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Change of feature importance with RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# get all availabe units and features\n",
    "units = df[\"unit\"].unique()\n",
    "features = df[\"Feature\"].unique()\n",
    "\n",
    "# choose unit and feature\n",
    "u = np.random.choice(units)\n",
    "#u = 84\n",
    "feature = \"Measurement 14\"\n",
    "\n",
    "\n",
    "# global line properties for each plot\n",
    "marker = 'o'\n",
    "markeredgecolor=\"white\"\n",
    "linewidth = 1\n",
    "model_colors = {\"xgb\": colormap.colors[0], \"rf\": colormap.colors[1], \"mlp\": colormap.colors[2], \"svm\": colormap.colors[3], \"ebm\": colormap.colors[4]}\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "# plot the measurement on the first subplot\n",
    "df = pd.read_csv(os.path.join(\"results\", explanation_files[0]))\n",
    "dfu = df.loc[(df[\"unit\"] == u)  & (df[\"Feature\"] == feature)]\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(dfu[\"RUL\"], dfu[\"Value\"], color=\"black\", marker=marker, markersize=4, markeredgecolor=markeredgecolor, linewidth=0.5)\n",
    "plt.xlim(dfu[\"RUL\"].max(), 0)\n",
    "plt.ylabel(\"Feature value\")\n",
    "plt.title(\"%s - unit %s\" % (feature, u), loc='left')\n",
    "# plt.xlabel(\"RUL (cycles)\")\n",
    "\n",
    "# build a global legend for models:\n",
    "legend_elements = [Line2D([0], [0], linewidth=linewidth, \n",
    "                          marker=marker, \n",
    "                          markeredgecolor=markeredgecolor, \n",
    "                          linestyle='-', \n",
    "                          color=col,\n",
    "                          label=key.upper()) for key, col in model_colors.items()]\n",
    "\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(0.5, 1.4), ncol=5, loc=\"center\", frameon=False)\n",
    "\n",
    "\n",
    "# make some global plots for each explanation\n",
    "for i, exp in enumerate([\"SHAP\", \"LIME\", \"EBM\"]):\n",
    "    plt.subplot(412+i)\n",
    "    plt.title(\"%s\" % exp, loc='left')\n",
    "    # plt.plot(dfu[\"RUL\"], np.zeros(dfu[\"RUL\"].shape[0]), color=\"black\")\n",
    "    plt.xlim(dfu[\"RUL\"].max(), 0)\n",
    "    plt.ylabel(\"Explanation score\")\n",
    "    \n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "# xlabel only on last plot\n",
    "plt.xlabel(\"RUL (cycles)\")\n",
    "\n",
    "for f in explanation_files:\n",
    "    # read each explanation file and extract selected unit / feature\n",
    "    df = pd.read_csv(os.path.join(\"results\", f))\n",
    "    dfu = df.loc[(df[\"unit\"] == u)  & (df[\"Feature\"] == feature)]\n",
    "      \n",
    "    # based on file name get model and explanation algorithm\n",
    "    mod = re.findall(\"xgb|rf|mlp|svm|ebm\", f)[0]\n",
    "    explain = re.findall(\"shap|lime|ebm\", f)[0]\n",
    "    if explain == \"shap\":\n",
    "        plt.subplot(412) \n",
    "    elif explain == 'lime':\n",
    "        plt.subplot(413)\n",
    "    elif explain == \"ebm\":\n",
    "        plt.subplot(414)\n",
    "    color = model_colors[mod]\n",
    "    plt.plot(dfu[\"RUL\"], dfu[\"Explain_Score\"], marker='o', color=color, linewidth=1., linestyle='-', markeredgecolor=\"white\", markersize=4,)\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "#plt.savefig(r\"plots/unit_explanations.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tsne = TSNE(**{\"n_components\": 2, \"perplexity\": 50})\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "\n",
    "def plot_2d(data, keys, titles, dim_model, save=False):\n",
    "    if type(keys) != list:\n",
    "        keys = [keys]\n",
    "    if type(titles) != list:\n",
    "        titles = [titles]\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1+len(keys), figsize=(10, 4))\n",
    "    data_plotted = False\n",
    "    \n",
    "    for i, (key, title) in enumerate(zip(keys, titles)):\n",
    "        data_explain = data[key]\n",
    "        X_2d_data = dim_model.fit_transform(data_explain[0])\n",
    "        X_2d_exp = dim_model.fit_transform(data_explain[1])\n",
    "    \n",
    "        if not data_plotted:\n",
    "            axes[0].set_title(\"Measurements\")\n",
    "            axes[0].scatter(X_2d_data[:, 0], X_2d_data[:, 1], s=8, c=data_explain[2], cmap='viridis', vmin=0, vmax=130)\n",
    "            data_plotted = True\n",
    "    \n",
    "        axes[i+1].set_title(title)\n",
    "        scatter = axes[i+1].scatter(X_2d_exp[:, 0], X_2d_exp[:, 1], s=8, c=data_explain[2], cmap='viridis', vmin=0, vmax=130)\n",
    "    \n",
    "#     for ax in axes.flat:\n",
    "#         ax.set_xticks([])\n",
    "#         ax.set_yticks([])\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel(\"PC1\")\n",
    "        ax.set_ylabel(\"PC2\")\n",
    "    \n",
    "    fig.colorbar(scatter, ax=axes.ravel().tolist(), label=\"RUL\", location=\"bottom\", pad=0.15, shrink=0.5, fraction=0.1, aspect=30)\n",
    "    #fig.tight_layout(rect=[0.1, 0, 0.8, 0.9])\n",
    "    if save:\n",
    "        dim_name = dim_model.__repr__().split(\"(\")[0].lower()\n",
    "        key_names = \"+\".join(keys)\n",
    "        plt.savefig(rf\"plots/{dim_name}-{key_names}.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_2d(data, [\"mlp_shap\", \"mlp_lime\"], [\"SHAP\", \"LIME\"], pca, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    print(key)\n",
    "    plot_2d(data, key, key, pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
